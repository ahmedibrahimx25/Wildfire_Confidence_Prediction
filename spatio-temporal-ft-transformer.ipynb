{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rtdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-23T08:48:39.155512Z",
     "iopub.status.busy": "2025-09-23T08:48:39.155337Z",
     "iopub.status.idle": "2025-09-23T08:48:42.841722Z",
     "shell.execute_reply": "2025-09-23T08:48:42.841031Z",
     "shell.execute_reply.started": "2025-09-23T08:48:39.155497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully concatenated all CSV files.\n",
      "Total rows in the combined DataFrame: 446165\n",
      "First 5 rows of the combined data:\n",
      "   latitude  longitude  bright_ti4  scan  track    acq_date  acq_time  \\\n",
      "0  19.51895   42.53688      336.56  0.39   0.36  2020-01-01      1018   \n",
      "1  18.70315   45.20741      338.43  0.40   0.37  2020-01-01      1018   \n",
      "2  22.52510   54.04734      301.22  0.70   0.75  2020-01-01      2106   \n",
      "3  22.52309   54.04637      301.26  0.70   0.75  2020-01-01      2106   \n",
      "4  22.60682   54.06105      296.30  0.70   0.75  2020-01-01      2106   \n",
      "\n",
      "  satellite instrument confidence  version  bright_ti5   frp daynight  type  \n",
      "0         N      VIIRS          n        2      310.35  2.15        D     2  \n",
      "1         N      VIIRS          n        2      306.76  2.25        D     0  \n",
      "2         N      VIIRS          n        2      284.61  1.54        N     2  \n",
      "3         N      VIIRS          n        2      284.52  1.04        N     0  \n",
      "4         N      VIIRS          n        2      285.11  1.22        N     2  \n",
      "\n",
      "Combined data has been saved to 'combined_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = r'/kaggle/input/firesett' \n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "try:\n",
    "    df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "    concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "    \n",
    "    print(\"Successfully concatenated all CSV files.\")\n",
    "    print(f\"Total rows in the combined DataFrame: {len(concatenated_df)}\")\n",
    "    print(\"First 5 rows of the combined data:\")\n",
    "    print(concatenated_df.head())\n",
    "\n",
    "    concatenated_df.to_csv(\"combined_data.csv\", index=False)\n",
    "    print(\"\\nCombined data has been saved to 'combined_data.csv'\")\n",
    "\n",
    "except ValueError:\n",
    "    print(\"No CSV files found in the specified directory or the files are empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T08:48:45.767126Z",
     "iopub.status.busy": "2025-09-23T08:48:45.766631Z",
     "iopub.status.idle": "2025-09-23T08:48:49.021494Z",
     "shell.execute_reply": "2025-09-23T08:48:49.020683Z",
     "shell.execute_reply.started": "2025-09-23T08:48:45.767101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw Sample DataFrame ---\n",
      "        latitude  longitude  bright_ti4  scan  track    acq_date  acq_time  \\\n",
      "0       19.51895   42.53688      336.56  0.39   0.36  2020-01-01      1018   \n",
      "1       18.70315   45.20741      338.43  0.40   0.37  2020-01-01      1018   \n",
      "2       22.52510   54.04734      301.22  0.70   0.75  2020-01-01      2106   \n",
      "3       22.52309   54.04637      301.26  0.70   0.75  2020-01-01      2106   \n",
      "4       22.60682   54.06105      296.30  0.70   0.75  2020-01-01      2106   \n",
      "...          ...        ...         ...   ...    ...         ...       ...   \n",
      "446160  23.95027   38.28195      299.08  0.51   0.66  2022-12-31      2357   \n",
      "446161  23.95134   38.28138      303.62  0.51   0.66  2022-12-31      2357   \n",
      "446162  22.55081   39.43581      301.06  0.65   0.73  2022-12-31      2358   \n",
      "446163  22.55534   39.43776      301.32  0.65   0.73  2022-12-31      2358   \n",
      "446164  20.71781   39.82619      299.05  0.75   0.77  2022-12-31      2358   \n",
      "\n",
      "       satellite instrument confidence  version  bright_ti5   frp daynight  \\\n",
      "0              N      VIIRS          n        2      310.35  2.15        D   \n",
      "1              N      VIIRS          n        2      306.76  2.25        D   \n",
      "2              N      VIIRS          n        2      284.61  1.54        N   \n",
      "3              N      VIIRS          n        2      284.52  1.04        N   \n",
      "4              N      VIIRS          n        2      285.11  1.22        N   \n",
      "...          ...        ...        ...      ...         ...   ...      ...   \n",
      "446160         N      VIIRS          n        2      281.57  0.54        N   \n",
      "446161         N      VIIRS          n        2      280.96  0.69        N   \n",
      "446162         N      VIIRS          n        2      286.03  2.00        N   \n",
      "446163         N      VIIRS          n        2      286.04  1.28        N   \n",
      "446164         N      VIIRS          n        2      286.63  0.80        N   \n",
      "\n",
      "        type  \n",
      "0          2  \n",
      "1          0  \n",
      "2          2  \n",
      "3          0  \n",
      "4          2  \n",
      "...      ...  \n",
      "446160     0  \n",
      "446161     0  \n",
      "446162     2  \n",
      "446163     2  \n",
      "446164     2  \n",
      "\n",
      "[446165 rows x 15 columns]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting preprocessing...\n",
      "Initial shape: (446165, 15)\n",
      "Shape after pruning columns: (446165, 10)\n",
      "Shape after dropping missing values: (446165, 10)\n",
      "Shape after dropping duplicates: (446165, 10)\n",
      "\n",
      "Starting feature engineering...\n",
      "Created 'hour', 'month', 'year' features and dropped originals.\n",
      "Shape after feature engineering: (446165, 11)\n",
      "\n",
      "Starting feature transformation and encoding...\n",
      "Numerical features to scale: ['latitude', 'longitude', 'bright_ti4', 'bright_ti5', 'frp']\n",
      "Categorical features to encode: ['type', 'daynight', 'hour', 'month', 'year']\n",
      "Fitting and transforming data with the preprocessor...\n",
      "\n",
      "Preprocessing complete. Final processed shape: (446165, 43)\n",
      "\n",
      "--- Processed DataFrame (Head) ---\n",
      "   latitude longitude bright_ti4 bright_ti5       frp type_0 type_2 type_3  \\\n",
      "0 -2.117275 -1.078664   1.006267   1.483752 -0.240364    0.0    1.0    0.0   \n",
      "1 -2.431381 -0.442905   1.112788   1.141123 -0.226822    1.0    0.0    0.0   \n",
      "2 -0.959823   1.66157   -1.00681  -0.972873 -0.322971    0.0    1.0    0.0   \n",
      "3 -0.960597  1.661339  -1.004531  -0.981462 -0.390681    1.0    0.0    0.0   \n",
      "4 -0.928359  1.664833  -1.287069  -0.925153 -0.366305    0.0    1.0    0.0   \n",
      "\n",
      "  daynight_D daynight_N  ... year_2015 year_2016 year_2017 year_2018  \\\n",
      "0        1.0        0.0  ...       0.0       0.0       0.0       0.0   \n",
      "1        1.0        0.0  ...       0.0       0.0       0.0       0.0   \n",
      "2        0.0        1.0  ...       0.0       0.0       0.0       0.0   \n",
      "3        0.0        1.0  ...       0.0       0.0       0.0       0.0   \n",
      "4        0.0        1.0  ...       0.0       0.0       0.0       0.0   \n",
      "\n",
      "  year_2019 year_2020 year_2021 year_2022 year_2023 confidence  \n",
      "0       0.0       1.0       0.0       0.0       0.0          0  \n",
      "1       0.0       1.0       0.0       0.0       0.0          0  \n",
      "2       0.0       1.0       0.0       0.0       0.0          0  \n",
      "3       0.0       1.0       0.0       0.0       0.0          0  \n",
      "4       0.0       1.0       0.0       0.0       0.0          0  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "\n",
      "--- Columns in Processed DataFrame ---\n",
      "Index(['latitude', 'longitude', 'bright_ti4', 'bright_ti5', 'frp', 'type_0',\n",
      "       'type_2', 'type_3', 'daynight_D', 'daynight_N', 'hour_0', 'hour_8',\n",
      "       'hour_9', 'hour_10', 'hour_11', 'hour_21', 'hour_22', 'hour_23',\n",
      "       'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
      "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12',\n",
      "       'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016',\n",
      "       'year_2017', 'year_2018', 'year_2019', 'year_2020', 'year_2021',\n",
      "       'year_2022', 'year_2023', 'confidence'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_flameguard_data(df: pd.DataFrame):\n",
    "    print(\"Starting preprocessing...\")\n",
    "    df_clean = df.copy()\n",
    "    print(f\"Initial shape: {df_clean.shape}\")\n",
    "\n",
    "    columns_to_drop = ['scan', 'track', 'satellite', 'instrument', 'version']\n",
    "    df_clean.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "    print(f\"Shape after pruning columns: {df_clean.shape}\")\n",
    "\n",
    "    df_clean.dropna(inplace=True)\n",
    "    print(f\"Shape after dropping missing values: {df_clean.shape}\")\n",
    "\n",
    "    df_clean.drop_duplicates(inplace=True)\n",
    "    print(f\"Shape after dropping duplicates: {df_clean.shape}\")\n",
    "\n",
    "    print(\"\\nStarting feature engineering...\")\n",
    "\n",
    "    df_clean['acq_date'] = pd.to_datetime(df_clean['acq_date'], errors='coerce')\n",
    "    df_clean.dropna(subset=['acq_date'], inplace=True)\n",
    "\n",
    "    # Convert acq_time to string, pad with zeros, then extract hour as integer\n",
    "    df_clean['hour'] = df_clean['acq_time'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "    df_clean['month'] = df_clean['acq_date'].dt.month\n",
    "    df_clean['year'] = df_clean['acq_date'].dt.year\n",
    "\n",
    "    df_clean.drop(columns=['acq_date', 'acq_time'], inplace=True)\n",
    "    print(\"Created 'hour', 'month', 'year' features and dropped originals.\")\n",
    "    print(f\"Shape after feature engineering: {df_clean.shape}\")\n",
    "\n",
    "    print(\"\\nStarting feature transformation and encoding...\")\n",
    "\n",
    "    numerical_features = ['latitude', 'longitude', 'bright_ti4', 'bright_ti5', 'frp']\n",
    "    categorical_features = ['type', 'daynight', 'hour', 'month', 'year']\n",
    "    \n",
    "    existing_numerical = [f for f in numerical_features if f in df_clean.columns]\n",
    "    existing_categorical = [f for f in categorical_features if f in df_clean.columns]\n",
    "    \n",
    "    print(f\"Numerical features to scale: {existing_numerical}\")\n",
    "    print(f\"Categorical features to encode: {existing_categorical}\")\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), existing_numerical),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), existing_categorical)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "     \n",
    "    print(\"Fitting and transforming data with the preprocessor...\")\n",
    "    processed_data = preprocessor.fit_transform(df_clean)\n",
    "\n",
    "    new_cat_names = preprocessor.named_transformers_['cat'].get_feature_names_out(existing_categorical)\n",
    "    \n",
    "    feature_columns_in_transformer = existing_numerical + existing_categorical\n",
    "    remainder_cols = [col for col in df_clean.columns if col not in feature_columns_in_transformer]\n",
    "\n",
    "    final_columns = existing_numerical + list(new_cat_names) + remainder_cols\n",
    "\n",
    "    processed_df = pd.DataFrame(processed_data, columns=final_columns, index=df_clean.index)\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete. Final processed shape: {processed_df.shape}\")\n",
    "    confidence_mapping = {\n",
    "        'n': 0,\n",
    "        'h': 1,\n",
    "        'l': 2\n",
    "    }\n",
    "\n",
    "    processed_df['confidence'] = processed_df['confidence'].map(confidence_mapping)\n",
    "\n",
    "    return processed_df, preprocessor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    raw_df = pd.read_csv('/kaggle/working/combined_data.csv')\n",
    "\n",
    "    print(\"--- Raw Sample DataFrame ---\")\n",
    "    print(raw_df)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    processed_df, preprocessor_obj = preprocess_flameguard_data(raw_df)\n",
    "\n",
    "    print(\"\\n--- Processed DataFrame (Head) ---\")\n",
    "    print(processed_df.head())\n",
    "    \n",
    "    print(\"\\n--- Columns in Processed DataFrame ---\")\n",
    "    print(processed_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T08:48:52.656027Z",
     "iopub.status.busy": "2025-09-23T08:48:52.655289Z",
     "iopub.status.idle": "2025-09-23T08:48:52.664885Z",
     "shell.execute_reply": "2025-09-23T08:48:52.664275Z",
     "shell.execute_reply.started": "2025-09-23T08:48:52.656001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence\n",
       "0    411085\n",
       "2     27024\n",
       "1      8056\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['confidence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:22:15.236201Z",
     "iopub.status.busy": "2025-09-23T09:22:15.235383Z",
     "iopub.status.idle": "2025-09-23T10:32:48.393612Z",
     "shell.execute_reply": "2025-09-23T10:32:48.392780Z",
     "shell.execute_reply.started": "2025-09-23T09:22:15.236170Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset loaded. Shape: (446165, 43)\n",
      "Verifying and casting data types to numeric...\n",
      "CONFIRMED: Found 42 numeric columns.\n",
      "Starting neighborhood feature engineering for 446165 points...\n",
      "Neighborhood feature engineering complete.\n",
      "Original train distribution: Counter({0: 328868, 2: 21619, 1: 6445})\n",
      "Resampled train distribution: Counter({0: 150000, 1: 75000, 2: 75000})\n",
      "Using device: cuda\n",
      "\n",
      "--- Training FT-Transformer ---\n",
      "Epoch [1/30] - Loss: 0.1101 - LR: 0.000100\n",
      "Epoch [2/30] - Loss: 0.0662 - LR: 0.000099\n",
      "Epoch [3/30] - Loss: 0.0602 - LR: 0.000098\n",
      "Epoch [4/30] - Loss: 0.0572 - LR: 0.000096\n",
      "Epoch [5/30] - Loss: 0.0541 - LR: 0.000093\n",
      "Epoch [6/30] - Loss: 0.0528 - LR: 0.000090\n",
      "Epoch [7/30] - Loss: 0.0511 - LR: 0.000087\n",
      "Epoch [8/30] - Loss: 0.0497 - LR: 0.000083\n",
      "Epoch [9/30] - Loss: 0.0486 - LR: 0.000079\n",
      "Epoch [10/30] - Loss: 0.0465 - LR: 0.000075\n",
      "Epoch [11/30] - Loss: 0.0454 - LR: 0.000070\n",
      "Epoch [12/30] - Loss: 0.0451 - LR: 0.000065\n",
      "Epoch [13/30] - Loss: 0.0431 - LR: 0.000060\n",
      "Epoch [14/30] - Loss: 0.0418 - LR: 0.000055\n",
      "Epoch [15/30] - Loss: 0.0411 - LR: 0.000050\n",
      "Epoch [16/30] - Loss: 0.0396 - LR: 0.000045\n",
      "Epoch [17/30] - Loss: 0.0381 - LR: 0.000040\n",
      "Epoch [18/30] - Loss: 0.0375 - LR: 0.000035\n",
      "Epoch [19/30] - Loss: 0.0361 - LR: 0.000030\n",
      "Epoch [20/30] - Loss: 0.0350 - LR: 0.000025\n",
      "Epoch [21/30] - Loss: 0.0340 - LR: 0.000021\n",
      "Epoch [22/30] - Loss: 0.0333 - LR: 0.000017\n",
      "Epoch [23/30] - Loss: 0.0324 - LR: 0.000013\n",
      "Epoch [24/30] - Loss: 0.0318 - LR: 0.000010\n",
      "Epoch [25/30] - Loss: 0.0308 - LR: 0.000007\n",
      "Epoch [26/30] - Loss: 0.0307 - LR: 0.000004\n",
      "Epoch [27/30] - Loss: 0.0303 - LR: 0.000002\n",
      "Epoch [28/30] - Loss: 0.0300 - LR: 0.000001\n",
      "Epoch [29/30] - Loss: 0.0297 - LR: 0.000000\n",
      "Epoch [30/30] - Loss: 0.0296 - LR: 0.000000\n",
      "\n",
      "--- Final Evaluation ---\n",
      "Accuracy: 0.9839\n",
      "Macro F1-Score: 0.9376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     82217\n",
      "           1       0.91      0.97      0.94      1611\n",
      "           2       0.81      0.97      0.88      5405\n",
      "\n",
      "    accuracy                           0.98     89233\n",
      "   macro avg       0.91      0.97      0.94     89233\n",
      "weighted avg       0.99      0.98      0.98     89233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Spatio-Temporal Feature Engineering + FT-Transformer (V11 - Official Constructor) ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "import time\n",
    "import gc\n",
    "import rtdl\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# --- Configuration ---\n",
    "K_NEIGHBORS = 32\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# --- Spatio-Temporal Feature Engineering Function ---\n",
    "def create_neighborhood_features(X_coords, X_features, k):\n",
    "    print(f\"Starting neighborhood feature engineering for {len(X_coords)} points...\")\n",
    "    coords_rad = np.deg2rad(X_coords[['latitude', 'longitude']].to_numpy(dtype=float))\n",
    "    if k >= len(coords_rad): k = max(1, len(coords_rad) - 1)\n",
    "    if k == 0: return pd.DataFrame()\n",
    "    tree = BallTree(coords_rad, metric=\"haversine\")\n",
    "    _, indices = tree.query(coords_rad, k=k+1)\n",
    "    neighbor_indices = indices[:, 1:]\n",
    "    features_np = X_features.to_numpy(dtype=float)\n",
    "    mean_features = np.zeros_like(features_np); std_features = np.zeros_like(features_np)\n",
    "    for i in range(len(X_coords)):\n",
    "        neighbor_feature_matrix = features_np[neighbor_indices[i]]\n",
    "        mean_features[i, :] = neighbor_feature_matrix.mean(axis=0)\n",
    "        std_features[i, :] = neighbor_feature_matrix.std(axis=0)\n",
    "    mean_df = pd.DataFrame(mean_features, index=X_features.index, columns=[f'mean_{col}_k{k}' for col in X_features.columns])\n",
    "    std_df = pd.DataFrame(std_features, index=X_features.index, columns=[f'std_{col}_k{k}' for col in X_features.columns])\n",
    "    X_rich = pd.concat([X_features, mean_df, std_df], axis=1)\n",
    "    print(\"Neighborhood feature engineering complete.\")\n",
    "    return X_rich\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "print(f\"Initial dataset loaded. Shape: {processed_df.shape}\")\n",
    "X = processed_df.drop('confidence', axis=1)\n",
    "y = processed_df['confidence']\n",
    "\n",
    "print(\"Verifying and casting data types to numeric...\")\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "X.fillna(0, inplace=True)\n",
    "print(f\"CONFIRMED: Found {len(X.select_dtypes(include=np.number).columns)} numeric columns.\")\n",
    "\n",
    "coord_cols = ['latitude', 'longitude']\n",
    "feature_cols = [col for col in X.columns if col not in coord_cols]\n",
    "X_coords = X[coord_cols]; X_features = X[feature_cols]\n",
    "X_rich = create_neighborhood_features(X_coords, X_features, k=K_NEIGHBORS)\n",
    "X_final = pd.concat([X_coords, X_rich], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Original train distribution: {Counter(y_train)}\")\n",
    "over = SMOTE(sampling_strategy={1: 75000, 2: 75000}, random_state=42)\n",
    "under = RandomUnderSampler(sampling_strategy={0: 150000}, random_state=42)\n",
    "pipeline = Pipeline(steps=[('o', over), ('u', under)])\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "print(f\"Resampled train distribution: {Counter(y_train_resampled)}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train_resampled.values, dtype=torch.long))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32), torch.tensor(y_test.values, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Model Initialization ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = rtdl.FTTransformer.make_baseline(\n",
    "    n_num_features=X_train_scaled.shape[1],\n",
    "    cat_cardinalities=None,\n",
    "    d_token=192,\n",
    "    n_blocks=4,\n",
    "    attention_dropout=0.2,\n",
    "    ffn_d_hidden=256,\n",
    "    ffn_dropout=0.1,\n",
    "    residual_dropout=0.0,\n",
    "    d_out=len(y.unique())\n",
    ").to(device)\n",
    "\n",
    "class_counts = y.value_counts().sort_index()\n",
    "dampened_weights_tensor = torch.sqrt(torch.tensor( (len(y) / (len(class_counts) * class_counts)).values, dtype=torch.float32)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=dampened_weights_tensor)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(\"\\n--- Training FT-Transformer ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(x_num=features, x_cat=None)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {avg_loss:.4f} - LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation ---\")\n",
    "model.eval()\n",
    "preds, labels_all = [], []\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(x_num=features, x_cat=None)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        labels_all.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(labels_all, preds):.4f}\")\n",
    "print(f\"Macro F1-Score: {f1_score(labels_all, preds, average='macro', zero_division=0):.4f}\")\n",
    "print(classification_report(labels_all, preds, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8287062,
     "sourceId": 13084202,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
